import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
import logging
import gc

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class QLoRATester:
    def __init__(self):
        self.base_model_path = "./gemma-2-9b-it-tr-new"
        self.adapter_path = "./gemma-matematik-qlora"
        self.model = None
        self.tokenizer = None
        
        # Memory temizle
        torch.cuda.empty_cache()
        gc.collect()
    
    def load_model(self):
        """Fine-tuned modeli yÃ¼kle - GPU'ya optimize edilmiÅŸ"""
        logger.info("Fine-tuned model yÃ¼kleniyor...")
        
        try:
            # Memory temizle
            torch.cuda.empty_cache()
            gc.collect()
            
            # Tokenizer yÃ¼kle
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.base_model_path,
                trust_remote_code=True
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # Quantization config - GPU memory iÃ§in optimize
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16
            )
            
            # Base model yÃ¼kle - GPU'ya optimize
            base_model = AutoModelForCausalLM.from_pretrained(
                self.base_model_path,
                quantization_config=bnb_config,
                device_map="cuda:0",  # Direkt GPU'ya
                trust_remote_code=True,
                torch_dtype=torch.bfloat16,
                low_cpu_mem_usage=True
            )
            
            # LoRA adapter'Ä±nÄ± yÃ¼kle
            self.model = PeftModel.from_pretrained(base_model, self.adapter_path)
            
            # Model'i eval moduna al
            self.model.eval()
            
            logger.info("âœ… Fine-tuned model GPU'da baÅŸarÄ±yla yÃ¼klendi!")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
            return False
    
    def generate_response(self, instruction, max_length=128):
        """Cevap Ã¼ret - GPU optimize"""
        try:
            # Memory temizle
            torch.cuda.empty_cache()
            
            # Gemma formatÄ±nda prompt hazÄ±rla
            prompt = f"<start_of_turn>user\n{instruction}<end_of_turn>\n<start_of_turn>model\n"
            
            # Tokenize et
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=256)
            
            # GPU'ya taÅŸÄ±
            inputs = {k: v.cuda() for k, v in inputs.items()}
            
            # Generate - GPU optimize
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_length,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            # Decode
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Assistant response'Ä±nÄ± Ã§Ä±kar
            if "<start_of_turn>model" in response:
                response = response.split("<start_of_turn>model")[-1].strip()
                # End token'Ä± kaldÄ±r
                if "<end_of_turn>" in response:
                    response = response.split("<end_of_turn>")[0].strip()
            
            # Memory temizle
            del outputs
            torch.cuda.empty_cache()
            
            return response
            
        except Exception as e:
            logger.error(f"âŒ Cevap Ã¼retme hatasÄ±: {e}")
            return f"Hata: {e}"
    
    def quick_test(self):
        """HÄ±zlÄ± test Ã¶rnekleri"""
        test_cases = [
            "DoÄŸal sayÄ±lar nedir?",
            "Kesirler nasÄ±l gÃ¶sterilir?",
            "Alan hesaplama nasÄ±l yapÄ±lÄ±r?"
        ]
        
        print("\nğŸ§ª HIZLI TEST Ã–RNEKLERÄ°:")
        print("=" * 50)
        
        for i, question in enumerate(test_cases, 1):
            print(f"\n{i}. Soru: {question}")
            response = self.generate_response(question)
            print(f"Cevap: {response}")
            print("-" * 30)
    
    def cleanup(self):
        """Memory temizle"""
        if self.model:
            del self.model
        if self.tokenizer:
            del self.tokenizer
        torch.cuda.empty_cache()
        gc.collect()

def main():
    print("ğŸ§ª QLoRA HIZLI TEST BaÅŸlatÄ±lÄ±yor...")
    print("=" * 50)
    
    tester = QLoRATester()
    
    try:
        if tester.load_model():
            tester.quick_test()
            
            # Ä°nteraktif test
            print("\nğŸ’¬ Ä°nteraktif Test Modu:")
            print("Ã‡Ä±kmak iÃ§in 'quit' yazÄ±n")
            print("-" * 30)
            
            while True:
                try:
                    user_input = input("\nSoru: ")
                    if user_input.lower() in ['quit', 'exit', 'Ã§Ä±k']:
                        break
                    
                    response = tester.generate_response(user_input)
                    print(f"Cevap: {response}")
                    
                except KeyboardInterrupt:
                    break
                except Exception as e:
                    print(f"Hata: {e}")
    except Exception as e:
        print(f"Ana hata: {e}")
    finally:
        tester.cleanup()
    
    print("\nâœ… Test tamamlandÄ±!")

if __name__ == "__main__":
    main() 