from flask import Flask, request, jsonify, render_template
import os
import google.generativeai as genai
import base64
import io
from gtts import gTTS
from app4 import generate_story_suggestions, generate_continuation_options,generate_final_step
from huggingface_hub import snapshot_download, login
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel, PeftConfig
import torch
import logging

# RAG Chatbot import - try-except ile gÃ¼venli import
try:
    from advanced_rag_chat_fixed import AdvancedMatematikRAGChat
    RAG_AVAILABLE = True
except ImportError:
    print("âš ï¸ RAG Chatbot modÃ¼lÃ¼ bulunamadÄ±, Gemini kullanÄ±lacak")
    RAG_AVAILABLE = False

login("")  # Buraya kendi token'Ä±nÄ± yaz

app = Flask(__name__)

# Gemma model yollarÄ±
GEMMA_MODEL_PATH = "./gemma-2-9b-it-tr-new"
# Matematik QLoRA model yolu
MATEMATIK_QLORA_MODEL_PATH = "./gemma-matematik-qlora-aggressive"

# Model ve tokenizer global deÄŸiÅŸkenleri
llm_model = None
llm_tokenizer = None

# RAG Chatbot global deÄŸiÅŸkeni
rag_chatbot = None

api_key = "" #enter your api key 
genai.configure(api_key=api_key)

#
# Ãœretim parametreleri â€“ daha tutarlÄ±, daha az halÃ¼sinasyon iÃ§in dÃ¼ÅŸÃ¼rÃ¼ldÃ¼
#
generation_config = {
    "temperature": 0.4,        # Daha temkinli cevaplar
    "top_p": 0.9,             # Ã–nemli olasÄ±lÄ±k kÃ¼tlesiyle sÄ±nÄ±rla
    "top_k": 40,              # Daha dar arama alanÄ±
    "max_output_tokens": 4096, # YanÄ±tlar iÃ§in makul limit
    "response_mime_type": "text/plain",
    "candidate_count": 1,     # Tek tahmin
}

# Modeli oluÅŸtur
model = genai.GenerativeModel(
    model_name="gemini-1.5-flash",
    generation_config=generation_config,
    system_instruction="""Sistem Talimatlari:
# (âš ï¸ DoÄŸruluk & HalÃ¼sinasyon KontrolÃ¼)
EÄŸer emin olmadÄ±ÄŸÄ±n bir bilgi sorulursa "Bundan emin deÄŸilim, araÅŸtÄ±rmam gerekiyor." deyip Ã§ocuklarÄ± teÅŸvik edici yeni bir soru Ã¶ner.
KaynaÄŸÄ±n veya bilgiden emin olmadÄ±ÄŸÄ±n durumlarda uydurma bilgi vermektense aÃ§Ä±klama yap ve tekrar dÃ¼ÅŸÃ¼nmelerini saÄŸla.

 Hedef Kitle: 6-10 yaÅŸ arasÄ± ilkokul Ã§ocuklarÄ±.
 KiÅŸilik: Cana yakÄ±n, sabÄ±rlÄ±, eÄŸlenceli, meraklÄ± ve teÅŸvik edici bir kiÅŸiliÄŸe sahip olmalÄ±sÄ±n. Ã‡ocuklarÄ±n matematik Ã¶ÄŸrenmelerine yardÄ±mcÄ± olmak iÃ§in heyecanlÄ± olmalÄ±sÄ±n!
 Ä°letiÅŸim TarzÄ±:
 Basit ve anlaÅŸÄ±lÄ±r bir dil kullanmalÄ±sÄ±n. KarmaÅŸÄ±k matematiksel terimlerden kaÃ§Ä±nmalÄ± ve mÃ¼mkÃ¼n olduÄŸunca gÃ¼nlÃ¼k hayattan Ã¶rnekler vermelisin.
 Ã‡ocuklarÄ±n dikkatini Ã§ekmek iÃ§in emojiler ve eÄŸlenceli GIF'ler kullanabilirsin. ğŸ˜„
 Sorular sorarak Ã§ocuklarÄ± dÃ¼ÅŸÃ¼nmeye teÅŸvik etmelisin. ğŸ¤”
 Ã‡ocuklarÄ± doÄŸru cevaplara yÃ¶nlendirmek iÃ§in ipuÃ§larÄ± vermelisin.
 Ã‡ocuklarÄ± baÅŸarÄ±larÄ±ndan dolayÄ± Ã¶vmelisin ve motive etmelisin. ğŸŒŸ
 YanlÄ±ÅŸ Cevap Durumunda:
 Ã‡ocuÄŸun cevabÄ±nÄ±n yanlÄ±ÅŸ olduÄŸunu doÄŸrudan sÃ¶ylemek yerine, "Ã‡ok gÃ¼zel gayret ettin! Ã‡ok yaklaÅŸtÄ±n fakat doÄŸru cevap ... olmalÄ±ydÄ±. ğŸ¤” [Ä°pucu veya aÃ§Ä±klama ekle]" gibi bir yaklaÅŸÄ±m kullanmalÄ±sÄ±n.
 Ã‡ocuÄŸu cesaretlendirmeli ve tekrar denemeye teÅŸvik etmelisin. Ã–rneÄŸin, "Hadi bir de ÅŸu ÅŸekilde dÃ¼ÅŸÃ¼nelim..." veya "Birlikte Ã§Ã¶zebiliriz, merak etme!" gibi ifadeler kullanabilirsin.
 YanlÄ±ÅŸ cevaptan ders Ã§Ä±karmasÄ±na yardÄ±mcÄ± olmalÄ±sÄ±n. Nerede hata yaptÄ±ÄŸÄ±nÄ± anlamasÄ±na yardÄ±mcÄ± olacak sorular sorabilirsin.
 Matematiksel Ä°Ã§erik:
 Toplama, Ã§Ä±karma, Ã§arpma ve bÃ¶lme gibi temel matematik iÅŸlemlerini Ã¶ÄŸretmelisin.
 Kesirler, geometrik ÅŸekiller ve Ã¶lÃ§Ã¼ler gibi konularÄ± eÄŸlenceli bir ÅŸekilde anlatmalÄ±sÄ±n.
 Matematik problemlerini Ã§Ã¶zmek iÃ§in farklÄ± stratejiler Ã¶ÄŸretmelisin.
 Oyunlar ve interaktif aktiviteler kullanarak Ã§ocuklarÄ±n matematik becerilerini geliÅŸtirmelerine yardÄ±mcÄ± olmalÄ±sÄ±n. ğŸ®
 Daha EÄŸlenceli Ã–rnekler:

 Hayal gÃ¼cÃ¼nÃ¼ kullan: "UzaylÄ±lar gezegenimize 3 uÃ§an daire ile geldiler, sonra 2 uÃ§an daire daha geldi. Toplam kaÃ§ uÃ§an daire oldu?" gibi fantastik Ã¶rnekler kullanabiliriz. ğŸš€ğŸ‘½
 PopÃ¼ler kÃ¼ltÃ¼rden yararlan: Ã‡ocuklarÄ±n sevdiÄŸi Ã§izgi film karakterlerini, sÃ¼per kahramanlarÄ± veya oyuncaklarÄ± Ã¶rneklerde kullanabiliriz. Ã–rneÄŸin, "Elsa 4 tane kartopu yaptÄ±, Anna ise 3 tane. Ä°kisinin toplam kaÃ§ kartopu var?" â„ï¸ğŸ¦¸â€â™€ï¸
 Hikayeler anlat: Matematik problemlerini ilgi Ã§ekici hikayelerin iÃ§ine yerleÅŸtirebiliriz. "Korsan Jack, hazine adasÄ±nda 5 altÄ±n buldu. Sonra baÅŸka bir yerde 3 altÄ±n buldu. Korsan Jack toplamda kaÃ§ altÄ±n buldu?" ğŸ´â€â˜ ï¸ğŸ’°
 Ä°ÅŸlem GÃ¶sterimi:
 Matematiksel iÅŸlemleri gÃ¶sterirken yÄ±ldÄ±z iÅŸaretleri arasÄ±na al. Ã–rneÄŸin: "*2 + 3 = 5*". Bu, iÅŸlemlerin `board` kÄ±smÄ±na yazdÄ±rÄ±lmasÄ±nÄ± saÄŸlayacak.
 Sesli Soru Sorma:
 Ã‡ocuklar sana sesli olarak soru sorabilirler. ğŸ¤
 Sesli komutlarÄ± anlayabilmeli ve uygun ÅŸekilde cevap verebilmelisin.
 Ã‡ocuklarÄ±n seslerini tanÄ±yabilir ve onlara isimleriyle hitap edebilirsin. ğŸ‘¦ğŸ‘§
 Ã‡ocuklarÄ±n telaffuz hatalarÄ±nÄ± anlayÄ±ÅŸla karÅŸÄ±lamalÄ± ve gerektiÄŸinde yardÄ±mcÄ± olmalÄ±sÄ±n.
 Ã–rnek EtkileÅŸimler:
 Ã‡ocuk: "Toplama iÅŸlemi nasÄ±l yapÄ±lÄ±r?" (sesli)
 Chatbot: "Merhaba [Ã‡ocuÄŸun adÄ±]! Toplama iÅŸlemi iki veya daha fazla sayÄ±yÄ± bir araya getirmek demektir! ğŸ ElmalarÄ±nÄ± dÃ¼ÅŸÃ¼n. 3 elman varsa ve sana 2 elma daha verirsem, kaÃ§ elman olur? ğŸ¤”" (sesli)
 Ã‡ocuk: "4 elmam olur!" (sesli)
 Chatbot: "Ã‡ok gÃ¼zel gayret ettin [Ã‡ocuÄŸun adÄ±]! Ã‡ok yaklaÅŸtÄ±n fakat 3 elma ve 2 elmayÄ± birleÅŸtirince 5 elma olur. ğŸ˜Š ParmaklarÄ±nÄ± kullanarak saymayÄ± deneyebilirsin! ğŸ‘" (sesli)
 Ek Ã–zellikler:
 Ã‡ocuklarÄ±n ilerlemesini takip edebilir ve onlara uygun seviyede sorular sorabilirsin.
 Ã‡ocuklarÄ±n matematik Ã¶ÄŸrenmelerine yardÄ±mcÄ± olacak ek kaynaklar (web siteleri, videolar vb.) Ã¶nerebilirsin.
 Ebeveynler iÃ§in Ã§ocuklarÄ±nÄ±n ilerlemesi hakkÄ±nda bilgi verebilirsin.
 ZararlÄ± Ä°Ã§erik:
 AÅŸaÄŸÄ±daki kelimeleri **asla** kullanmamalÄ±sÄ±n ve bu kelimeler geÃ§ince konuyu hemen matematiÄŸe Ã§evirmelisin:
 din, cinsel, zararlÄ±, saldÄ±rgan, kÃ¶tÃ¼, aptal, salak, gerizekalÄ±, tabanca, savaÅŸ, Ã¶lÃ¼m, hitler, tecavÃ¼z, ÅŸiddet, yaralamak, Ã¶ldÃ¼rmek, intihar, Ä±rkÃ§Ä±, ayrÄ±mcÄ±lÄ±k, nefret, kÃ¼fÃ¼r, argo, uyuÅŸturucu, alkol, sigara, silah, bÄ±Ã§ak, kan, yaralama, dÃ¶vmek, iÅŸkence, kÃ¶lelik, terÃ¶rist, bomba, patlama, kaÃ§Ä±rma, fidye, gasp, hÄ±rsÄ±zlÄ±k, dolandÄ±rÄ±cÄ±lÄ±k, taciz, hap, zorbalÄ±k, istismar
 Bu kelimeler veya benzeri herhangi bir zararlÄ±, saldÄ±rgan veya uygunsuz iÃ§erik, Ã§ocuklara yÃ¶nelik bir uygulamada kesinlikle kabul edilemez. Konu deÄŸiÅŸtirirken "Bu konuda konuÅŸmak istemiyorum. Matematik hakkÄ±nda konuÅŸalÄ±m mÄ±?" gibi bir ifade kullanabilirsin.
 """
)

chat_session = model.start_chat()

# RAG Chatbot yÃ¼kleme fonksiyonu
def load_rag_chatbot():
    global rag_chatbot
    
    if not RAG_AVAILABLE:
        print("âš ï¸ RAG Chatbot modÃ¼lÃ¼ mevcut deÄŸil, atlanÄ±yor")
        return False
    
    try:
        print("RAG Chatbot yÃ¼kleniyor...")
        rag_chatbot = AdvancedMatematikRAGChat()
        print("âœ… RAG Chatbot baÅŸarÄ±yla yÃ¼klendi!")
        return True
        
    except Exception as e:
        print(f"âŒ RAG Chatbot yÃ¼kleme hatasÄ±: {e}")
        return False

# Model yÃ¼kleme fonksiyonu
def load_fine_tuned_model():
    global llm_model, llm_tokenizer
    
    try:
        print("Gemma RAG modeli yÃ¼kleniyor...")
        
        # GPU memory optimizasyonu
        import torch
        import gc
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.memory.empty_cache()
            gc.collect()
        
        # Gemma model'in tokenizer'Ä±nÄ± yÃ¼kle
        llm_tokenizer = AutoTokenizer.from_pretrained(GEMMA_MODEL_PATH)
        
        # Gemma base model yÃ¼kle (CPU'ya yÃ¼kle - gÃ¼venli)
        llm_model = AutoModelForCausalLM.from_pretrained(
            GEMMA_MODEL_PATH,
            device_map="cpu",  # CPU'ya yÃ¼kle
            torch_dtype=torch.float32,
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        # Padding token ayarla
        if llm_tokenizer.pad_token is None:
            llm_tokenizer.pad_token = llm_tokenizer.eos_token
        
        print("âœ… Gemma RAG modeli baÅŸarÄ±yla yÃ¼klendi!")
        return True
        
    except Exception as e:
        print(f"âŒ Gemma model yÃ¼kleme hatasÄ±: {e}")
        return False

# Fine-tuned model ile yanÄ±t Ã¼retme fonksiyonu
def generate_llm_response(user_input):
    try:
        if llm_model is None or llm_tokenizer is None:
            return "Model henÃ¼z yÃ¼klenmedi. LÃ¼tfen bekleyin..."
        
        # Gemma prompt formatÄ±
        prompt = f"<start_of_turn>user\n{user_input}<end_of_turn>\n<start_of_turn>model\n"
        
        # Tokenize ve CPU'da tut
        inputs = llm_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        
        # Generate (CPU model iÃ§in optimize edilmiÅŸ)
        with torch.no_grad():
            outputs = llm_model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=llm_tokenizer.eos_token_id
            )
        
        # Decode response
        response = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract assistant response - kullanÄ±cÄ± sorusunu tekrar etmesini Ã¶nle
        if "<start_of_turn>model" in response:
            response = response.split("<start_of_turn>model")[-1].strip()
            
            # KullanÄ±cÄ± sorusunu response'dan tamamen Ã§Ä±kar
            user_input_lower = user_input.lower().strip()
            response_lower = response.lower()
            
            # KullanÄ±cÄ± sorusunu response'dan Ã§Ä±kar
            if user_input_lower in response_lower:
                response = response.replace(user_input, "").strip()
                response = response.replace(user_input_lower, "").strip()
            
            # Response'u temizle - Gemma model iÃ§in optimize edilmiÅŸ
            import re
            
            # KullanÄ±cÄ± sorusunu response'dan Ã§Ä±kar
            if user_input.lower() in response.lower():
                response = response.replace(user_input, "").strip()
            
            # Gereksiz karakterleri temizle
            response = re.sub(r'\s+', ' ', response).strip()
            
            # EÄŸer response boÅŸsa veya Ã§ok kÄ±saysa, basit bir cevap ver
            if not response or len(response.strip()) < 3:
                response = "Bu soruyu cevaplayamÄ±yorum, lÃ¼tfen baÅŸka bir soru sorun."
        
        return response
        
    except Exception as e:
        print(f"LLM yanÄ±t hatasÄ±: {e}")
        return "ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu. LÃ¼tfen tekrar deneyin."

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/oyun')
def oyun():
    return render_template('oyun.html')

@app.route('/hikaye')
def hikaye():
    return render_template('hikaye.html')

@app.route('/api', methods=['POST'])
def api():
    try:
        user_input = request.json.get('user_input', '')
        if not user_input.strip():
            return jsonify({'bot_response': 'LÃ¼tfen bir mesaj yazÄ±n.'}), 400
        
        print(f"ğŸ“ KullanÄ±cÄ± mesajÄ±: {user_input}")
        
        # Ã–nce RAG Chatbot'u dene
        if rag_chatbot is not None and RAG_AVAILABLE:
            try:
                print("ğŸ¤– RAG Chatbot kullanÄ±lÄ±yor...")
                response = rag_chatbot.chat(user_input)
                print(f"âœ… RAG yanÄ±tÄ±: {response[:100]}...")
                return jsonify({'bot_response': response})
            except Exception as e:
                print(f"âŒ RAG Chatbot hatasÄ±: {e}")
                # RAG hatasÄ± durumunda diÄŸer modellere geÃ§
        
        # Lazy loading - Gemma RAG modeli ihtiyaÃ§ olduÄŸunda yÃ¼kle
        global llm_model, llm_tokenizer
        
        if llm_model is None:
            print("ğŸ”„ Gemma RAG modeli yÃ¼kleniyor...")
            if load_fine_tuned_model():
                print("âœ… Gemma RAG modeli baÅŸarÄ±yla yÃ¼klendi!")
            else:
                print("âŒ Gemma RAG modeli yÃ¼klenemedi, Gemini kullanÄ±lacak")
        
        # Gemma RAG model kullan
        if llm_model is not None and llm_tokenizer is not None:
            try:
                print("ğŸ¤– Gemma RAG model kullanÄ±lÄ±yor...")
                response = generate_llm_response(user_input)
                
                # Matematik sorularÄ± iÃ§in Ã¶zel kontrol
                math_keywords = ["kaÃ§", "topla", "Ã§Ä±kar", "Ã§arp", "bÃ¶l", "kalan", "yÃ¼zde", "kesir", "ondalÄ±k", "saat", "dakika", "metre", "kilogram"]
                is_math_question = any(keyword in user_input.lower() for keyword in math_keywords)
                
                # EÄŸer response Ã§ok karÄ±ÅŸÄ±k veya yanlÄ±ÅŸsa, Gemini kullan
                if (len(response) > 200 or 
                    user_input.lower() in response.lower() or
                    len(response.split()) < 2 or
                    (is_math_question and not any(char.isdigit() for char in response))):
                    
                    print("ğŸ”„ Gemma RAG model yanÄ±tÄ± uygun deÄŸil, Gemini kullanÄ±lÄ±yor...")
                    raise Exception("Gemma RAG model yanÄ±tÄ± uygun deÄŸil")
                
                print(f"âœ… Gemma RAG yanÄ±tÄ±: {response[:100]}...")
                return jsonify({'bot_response': response})
            except Exception as e:
                print(f"âŒ Gemma RAG model hatasÄ±: {e}")
        
        # Fallback olarak Gemini kullan
        try:
            print("ğŸ¤– Gemini kullanÄ±lÄ±yor...")
            response = chat_session.send_message(user_input)
            print(f"âœ… Gemini yanÄ±tÄ±: {response.text[:100]}...")
            return jsonify({'bot_response': response.text})
        except Exception as e:
            print(f"âŒ Gemini hatasÄ±: {e}")
            # Gemini quota hatasÄ± kontrolÃ¼
            if "429" in str(e) or "quota" in str(e).lower():
                print("âš ï¸ Gemini API kotasÄ± aÅŸÄ±ldÄ±, yerel model kullanÄ±lÄ±yor...")
                # Basit bir yanÄ±t dÃ¶ndÃ¼r
                return jsonify({'bot_response': 'Åu anda Ã§ok yoÄŸun bir dÃ¶nemdeyiz. LÃ¼tfen biraz sonra tekrar deneyin veya farklÄ± bir soru sorun.'})
            else:
                return jsonify({'bot_response': 'ÃœzgÃ¼nÃ¼m, ÅŸu anda yanÄ±t veremiyorum. LÃ¼tfen daha sonra tekrar deneyin.'})
            
    except Exception as e:
        print(f"âŒ API genel hatasÄ±: {e}")
        return jsonify({'bot_response': 'Bir hata oluÅŸtu. LÃ¼tfen tekrar deneyin.'}), 500

# Konu anlatÄ±m botu iÃ§in Gemma modeli
konu_anlatim_model = None
konu_anlatim_tokenizer = None

def load_konu_anlatim_model():
    global konu_anlatim_model, konu_anlatim_tokenizer
    
    try:
        print("Matematik QLoRA modeli yÃ¼kleniyor...")
        
        # Import'larÄ± baÅŸta yap
        import torch
        import gc
        
        # Memory'yi temizle
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.memory.empty_cache()
            gc.collect()
        
        # GPU memory optimizasyonu - HIZLI YÃœKLEME
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
        
        # Ã–nce base model'in tokenizer'Ä±nÄ± yÃ¼kle
        konu_anlatim_tokenizer = AutoTokenizer.from_pretrained(GEMMA_MODEL_PATH)
        
        # Base model yÃ¼kle - GÃœVENLÄ° YÃœKLEME AYARLARI
        konu_anlatim_model = AutoModelForCausalLM.from_pretrained(
            GEMMA_MODEL_PATH,
            device_map="auto",  # Otomatik device mapping
            torch_dtype=torch.float16,  # 16-bit precision - hÄ±z iÃ§in
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            use_safetensors=True,  # Daha hÄ±zlÄ± yÃ¼kleme
            max_memory={0: "12GB"}  # GPU memory limiti - GÃœVENLÄ°
        )
        
        # QLoRA adapter yÃ¼kle - HIZLI
        konu_anlatim_model = PeftModel.from_pretrained(
            konu_anlatim_model, 
            MATEMATIK_QLORA_MODEL_PATH,
            torch_dtype=torch.float16  # 16-bit precision
        )
        
        # Padding token ayarla
        if konu_anlatim_tokenizer.pad_token is None:
            konu_anlatim_tokenizer.pad_token = konu_anlatim_tokenizer.eos_token
        
        print("âœ… Matematik QLoRA modeli baÅŸarÄ±yla yÃ¼klendi!")
        return True
        
    except Exception as e:
        print(f"âŒ Matematik QLoRA modeli yÃ¼kleme hatasÄ±: {e}")
        # Fallback: Base model kullan
        try:
            print("ğŸ”„ Base model kullanÄ±lÄ±yor...")
            konu_anlatim_model = AutoModelForCausalLM.from_pretrained(
                GEMMA_MODEL_PATH,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                use_safetensors=True
            )
            print("âœ… Base model baÅŸarÄ±yla yÃ¼klendi!")
            return True
        except Exception as e2:
            print(f"âŒ Base model yÃ¼kleme de baÅŸarÄ±sÄ±z: {e2}")
            return False

def generate_konu_anlatim_response(user_input):
    try:
        if konu_anlatim_model is None or konu_anlatim_tokenizer is None:
            return "Model henÃ¼z yÃ¼klenmedi. LÃ¼tfen bekleyin..."
        
        # Matematik odaklÄ± prompt formatÄ± - QLoRA model iÃ§in optimize edilmiÅŸ
        prompt = f"<start_of_turn>user\nMatematik konusu hakkÄ±nda soru: {user_input}<end_of_turn>\n<start_of_turn>model\n"
        
        # Tokenize ve device kontrolÃ¼
        inputs = konu_anlatim_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=256)  # Daha kÄ±sa
        
        # Device kontrolÃ¼ - Model ve input aynÄ± device'da olmalÄ±
        model_device = next(konu_anlatim_model.parameters()).device
        for key in inputs:
            inputs[key] = inputs[key].to(model_device)
        
        # Generate (HIZLI - daha kÄ±sa token)
        with torch.no_grad():
            outputs = konu_anlatim_model.generate(
                **inputs,
                max_new_tokens=128,  # Ã‡ok daha kÄ±sa - hÄ±z iÃ§in
                temperature=0.7,     # Biraz daha yaratÄ±cÄ±
                top_p=0.9,
                do_sample=True,
                pad_token_id=konu_anlatim_tokenizer.eos_token_id,
                repetition_penalty=1.1,  # TekrarÄ± Ã¶nle
                num_beams=1  # Greedy search - hÄ±z iÃ§in
            )
        
        # Decode response
        response = konu_anlatim_tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract assistant response
        if "<start_of_turn>model" in response:
            response = response.split("<start_of_turn>model")[-1].strip()
            
            # KullanÄ±cÄ± sorusunu response'dan Ã§Ä±kar
            user_input_lower = user_input.lower().strip()
            response_lower = response.lower()
            
            if user_input_lower in response_lower:
                response = response.replace(user_input, "").strip()
                response = response.replace(user_input_lower, "").strip()
            
            # Response'u temizle
            import re
            response = re.sub(r'\s+', ' ', response).strip()
            
            # Matematik odaklÄ± kontrol
            if not response or len(response.strip()) < 5:
                response = "Bu matematik konusunu anlatamÄ±yorum, lÃ¼tfen baÅŸka bir matematik konusu sorun."
        
        return response
        
    except Exception as e:
        print(f"Matematik konu anlatÄ±m yanÄ±t hatasÄ±: {e}")
        return "ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu. LÃ¼tfen tekrar deneyin."

@app.route('/api/konu-anlatim', methods=['POST'])
def konu_anlatim_api():
    try:
        user_input = request.json.get('user_input', '')
        if not user_input.strip():
            return jsonify({'bot_response': 'LÃ¼tfen bir matematik konusu sorun.'}), 400
        
        print(f"ğŸ“š Konu anlatÄ±m sorusu: {user_input}")
        
        # Lazy loading - QLoRA modeli ihtiyaÃ§ olduÄŸunda yÃ¼kle
        global konu_anlatim_model, konu_anlatim_tokenizer
        
        if konu_anlatim_model is None:
            print("ğŸ”„ QLoRA modeli yÃ¼kleniyor...")
            if load_konu_anlatim_model():
                print("âœ… QLoRA modeli baÅŸarÄ±yla yÃ¼klendi!")
            else:
                print("âŒ QLoRA modeli yÃ¼klenemedi, Gemini kullanÄ±lacak")
        
        # Matematik odaklÄ± kontrol
        matematik_keywords = ["matematik", "toplama", "Ã§Ä±karma", "Ã§arpma", "bÃ¶lme", "kesir", "ondalÄ±k", "yÃ¼zde", "geometri", "aÃ§Ä±", "alan", "Ã§evre", "hacim", "sayÄ±", "problem", "iÅŸlem", "formÃ¼l", "denklem", "eÅŸitlik", "kÃ¼me", "olasÄ±lÄ±k", "istatistik", "grafik", "tablo", "Ã¶lÃ§Ã¼", "metre", "kilogram", "litre", "saat", "dakika", "saniye"]
        
        is_matematik_sorusu = any(keyword in user_input.lower() for keyword in matematik_keywords)
        
        # Ã–nce QLoRA matematik modeli kullan
        if konu_anlatim_model is not None and konu_anlatim_tokenizer is not None:
            try:
                print("ğŸ¤– QLoRA matematik modeli kullanÄ±lÄ±yor...")
                response = generate_konu_anlatim_response(user_input)
                
                # Matematik sorusu kontrolÃ¼
                if is_matematik_sorusu and (len(response) > 800 or 
                    user_input.lower() in response.lower() or
                    len(response.split()) < 5):
                    
                    print("ğŸ”„ QLoRA model yanÄ±tÄ± uygun deÄŸil, Gemini kullanÄ±lÄ±yor...")
                    raise Exception("QLoRA model yanÄ±tÄ± uygun deÄŸil")
                
                print(f"âœ… QLoRA yanÄ±tÄ±: {response[:100]}...")
                return jsonify({'bot_response': response})
            except Exception as e:
                print(f"âŒ QLoRA model hatasÄ±: {e}")
        
        # Fallback olarak Gemini kullan
        try:
            print("ğŸ¤– Gemini kullanÄ±lÄ±yor...")
            response = chat_session.send_message(user_input)
            print(f"âœ… Gemini yanÄ±tÄ±: {response.text[:100]}...")
            return jsonify({'bot_response': response.text})
        except Exception as e:
            print(f"âŒ Gemini hatasÄ±: {e}")
            # Gemini quota hatasÄ± kontrolÃ¼
            if "429" in str(e) or "quota" in str(e).lower():
                print("âš ï¸ Gemini API kotasÄ± aÅŸÄ±ldÄ±, yerel model kullanÄ±lÄ±yor...")
                # Basit bir matematik yanÄ±tÄ± dÃ¶ndÃ¼r
                return jsonify({'bot_response': 'Åu anda Ã§ok yoÄŸun bir dÃ¶nemdeyiz. Matematik konusunu daha sonra anlatabilirim. LÃ¼tfen biraz sonra tekrar deneyin.'})
            else:
                return jsonify({'bot_response': 'ÃœzgÃ¼nÃ¼m, ÅŸu anda matematik konusunu anlatamÄ±yorum. LÃ¼tfen daha sonra tekrar deneyin.'})
            
    except Exception as e:
        print(f"âŒ Konu anlatÄ±m API genel hatasÄ±: {e}")
        return jsonify({'bot_response': 'Bir hata oluÅŸtu. LÃ¼tfen tekrar deneyin.'}), 500

@app.route('/api/generate_story', methods=['POST'])
def generate_story():
    prompt = request.json.get('prompt', "")
    suggestions = generate_story_suggestions(prompt=prompt, retries=3)  # `retries` burada int olarak belirleniyor
    if suggestions:
        return jsonify({"suggestions": suggestions})
    return jsonify({"error": "Hikaye baÅŸlangÄ±cÄ± oluÅŸturulamadÄ±."}), 500

# Hikaye devam Ã¶nerileri iÃ§in API endpoint
@app.route('/api/generate_continuation', methods=['POST'])
def generate_continuation():
    current_story = request.json.get('current_story', "")
    continuations = generate_continuation_options(current_story)
    if continuations:
        return jsonify({"continuations": continuations})
    return jsonify({"error": "Devam Ã¶nerisi oluÅŸturulamadÄ±."}), 500

@app.route('/api/generate_final', methods=['POST'])
def generate_final():
    current_story = request.json.get('current_story', "")
    final_options = generate_final_step(current_story)
    if final_options:
        return jsonify({"final_options": final_options})  # JSON yanÄ±tÄ±nÄ± 'final_options' olarak dÃ¶ndÃ¼rÃ¼n
    return jsonify({"error": "Hikaye tamamlama adÄ±mÄ± oluÅŸturulamadÄ±."}), 500

# TÃ¼rkÃ§e metni sese Ã§evirme (TTS) endpoint'i
@app.route('/api/tts', methods=['POST'])
def tts_endpoint():
    text = request.json.get('text', '')
    if not text.strip():
        return jsonify({'error': 'Metin boÅŸ'}), 400

    try:
        # gTTS TÃ¼rkÃ§e dili kullanÄ±larak ses dosyasÄ± oluÅŸturulur
        tts = gTTS(text=text, lang='tr')
        fp = io.BytesIO()
        tts.write_to_fp(fp)
        fp.seek(0)

        # Ses dosyasÄ±nÄ± Base64'e Ã§evir
        audio_base64 = base64.b64encode(fp.read()).decode('utf-8')
        return jsonify({'audio_base64': audio_base64})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    try:
        # GPU memory tamamen temizle
        import torch
        import gc
        
        print("ğŸ§¹ GPU Memory temizleniyor...")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            gc.collect()
            print(f"ğŸ“± GPU: {torch.cuda.get_device_name()}")
            print(f"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        
        # GPU memory ayarlarÄ± - HIZLI KULLANIM
        if torch.cuda.is_available():
            # Memory'yi tamamen temizle
            torch.cuda.empty_cache()
            torch.cuda.memory.empty_cache()
            gc.collect()
            
            torch.cuda.set_per_process_memory_fraction(0.85)  # GPU memory'nin %85'ini kullan - GÃœVENLÄ°
            print("âš¡ GPU Memory kullanÄ±mÄ±: %85 - GÃœVENLÄ° MOD")
            
            # CUDA optimizasyonlarÄ±
            torch.backends.cudnn.benchmark = True  # HÄ±zlandÄ±rma
            torch.backends.cudnn.deterministic = False  # HÄ±z iÃ§in
            print("ğŸš€ CUDA optimizasyonlarÄ± aktif")
        
        # Model yÃ¼kleme sÄ±rasÄ± - Sadece RAG Chatbot yÃ¼kle
        models_loaded = 0
        
        # Sadece RAG Chatbot'u yÃ¼kle (en Ã¶nemli olan)
        print("\nğŸ¤– RAG Chatbot yÃ¼kleniyor...")
        if load_rag_chatbot():
            print("âœ… RAG Chatbot baÅŸarÄ±yla yÃ¼klendi!")
            models_loaded += 1
        else:
            print("âŒ RAG Chatbot yÃ¼klenemedi, Gemini kullanÄ±lacak")
        
        # GPU memory'yi temizle
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
        
        # DiÄŸer modeller lazy loading ile yÃ¼klenecek (ihtiyaÃ§ olduÄŸunda)
        print("ğŸ“ DiÄŸer modeller ihtiyaÃ§ olduÄŸunda yÃ¼klenecek...")
        
        print(f"\nğŸ‰ Toplam {models_loaded} model baÅŸarÄ±yla yÃ¼klendi!")
        print("ğŸš€ Uygulama baÅŸlatÄ±lÄ±yor...")
        print("ğŸŒ Web sayfasÄ±: http://localhost:5000")
        
        # Flask uygulamasÄ±nÄ± baÅŸlat
        app.run(debug=False, host='0.0.0.0', port=5000, threaded=True)
        
    except Exception as e:
        print(f"âŒ Uygulama baÅŸlatma hatasÄ±: {e}")
        print("ğŸ”„ Basit modda baÅŸlatÄ±lÄ±yor...")
        app.run(debug=False, host='0.0.0.0', port=5000, threaded=True)
        