# ğŸ‡ Matematik QLoRA Fine-tuning

Bu proje, Gemma-2-9b-it-tr-new modelini matematik konularÄ± verileriyle QLoRA (Quantized Low-Rank Adaptation) kullanarak fine-tuning eder.

## ğŸ“‹ Gereksinimler

- Python 3.8+
- CUDA uyumlu GPU (en az 8GB VRAM Ã¶nerilir)
- 16GB+ RAM

## ğŸš€ Kurulum

1. **Gerekli kÃ¼tÃ¼phaneleri yÃ¼kle:**
```bash
pip install -r requirements_qlora.txt
```

2. **EÄŸitimi baÅŸlat:**
```bash
# Windows iÃ§in
start_qlora_training.bat

# Veya manuel olarak
python train_qlora_gemma.py
```

## ğŸ“ Dosya YapÄ±sÄ±

```
â”œâ”€â”€ train_qlora_gemma.py          # Ana eÄŸitim dosyasÄ±
â”œâ”€â”€ test_qlora_model.py           # Model test dosyasÄ±
â”œâ”€â”€ requirements_qlora.txt         # Gerekli kÃ¼tÃ¼phaneler
â”œâ”€â”€ start_qlora_training.bat      # Windows baÅŸlatma dosyasÄ±
â”œâ”€â”€ gemma-2-9b-it-tr-new/        # Base model
â”œâ”€â”€ gemma-matematik-qlora/        # Fine-tuned model (eÄŸitim sonrasÄ±)
â””â”€â”€ mat*.json                     # Matematik konularÄ± verileri
```

## âš™ï¸ KonfigÃ¼rasyon

### QLoRA AyarlarÄ±
- **Rank (r):** 16
- **Alpha:** 32
- **Dropout:** 0.05
- **Target Modules:** q_proj, v_proj, k_proj, o_proj, gate_proj, up_proj, down_proj

### EÄŸitim AyarlarÄ±
- **Epochs:** 3
- **Batch Size:** 2
- **Gradient Accumulation:** 4
- **Learning Rate:** 2e-4
- **Warmup Steps:** 100

### Quantization
- **4-bit quantization** (BitsAndBytes)
- **Double quantization** aktif
- **NF4** quantization type
- **bfloat16** compute dtype

## ğŸ“Š Veri FormatÄ±

EÄŸitim verileri ÅŸu formatta olmalÄ±dÄ±r:
```json
[
  {
    "instruction": "Soru veya talimat",
    "output": "Beklenen cevap"
  }
]
```

## ğŸ§ª Model Testi

EÄŸitim tamamlandÄ±ktan sonra modeli test etmek iÃ§in:

```bash
python test_qlora_model.py
```

## ğŸ’¾ Model Kaydetme

Fine-tuned model `gemma-matematik-qlora/` dizinine kaydedilir:
- `adapter_config.json` - LoRA konfigÃ¼rasyonu
- `adapter_model.safetensors` - LoRA aÄŸÄ±rlÄ±klarÄ±
- `tokenizer.json` - Tokenizer dosyalarÄ±

## ğŸ”§ KullanÄ±m

### EÄŸitim SonrasÄ± Model YÃ¼kleme

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Base model yÃ¼kle
base_model = AutoModelForCausalLM.from_pretrained("./gemma-2-9b-it-tr-new")
tokenizer = AutoTokenizer.from_pretrained("./gemma-2-9b-it-tr-new")

# LoRA adapter'Ä±nÄ± yÃ¼kle
model = PeftModel.from_pretrained(base_model, "./gemma-matematik-qlora")
```

### Cevap Ãœretme

```python
prompt = "<start_of_turn>user\nMatematik sorusu<end_of_turn>\n<start_of_turn>model\n"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=256)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
```

## ğŸ“ˆ Performans Ä°puÃ§larÄ±

1. **GPU Memory:** EÄŸitim sÄ±rasÄ±nda GPU memory kullanÄ±mÄ±nÄ± izleyin
2. **Gradient Checkpointing:** BÃ¼yÃ¼k modeller iÃ§in aktif
3. **Mixed Precision:** fp16 kullanarak hÄ±zlandÄ±rma
4. **Batch Size:** GPU memory'ye gÃ¶re ayarlayÄ±n

## ğŸ› Sorun Giderme

### YaygÄ±n Hatalar

1. **CUDA Out of Memory:**
   - Batch size'Ä± dÃ¼ÅŸÃ¼rÃ¼n
   - Gradient accumulation steps'i artÄ±rÄ±n
   - Model'i CPU'ya yÃ¼kleyin

2. **Import Errors:**
   - `pip install -r requirements_qlora.txt` Ã§alÄ±ÅŸtÄ±rÄ±n
   - CUDA versiyonunu kontrol edin

3. **Training Loss NaN:**
   - Learning rate'i dÃ¼ÅŸÃ¼rÃ¼n
   - Gradient clipping ekleyin

## ğŸ“ Notlar

- EÄŸitim sÃ¼resi GPU'ya baÄŸlÄ± olarak 2-6 saat sÃ¼rebilir
- Model boyutu yaklaÅŸÄ±k 18GB (base model) + ~100MB (LoRA weights)
- EÄŸitim sonrasÄ± model matematik konularÄ±nda daha iyi performans gÃ¶sterecek

## ğŸ¯ SonuÃ§

Bu fine-tuning iÅŸlemi sonrasÄ±nda model:
- âœ… Matematik konularÄ±nÄ± daha iyi anlayacak
- âœ… TÃ¼rkÃ§e matematik terimlerini daha iyi kullanacak
- âœ… SÄ±nÄ±f seviyesine uygun cevaplar Ã¼retecek
- âœ… EÄŸitim verilerindeki formatÄ± takip edecek 