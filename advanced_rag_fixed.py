import json
import os
import pickle
from typing import List, Dict, Any
import numpy as np
from sentence_transformers import SentenceTransformer
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import faiss
import logging
import re
import gc

# CUDA memory optimizasyonlarÄ± - RTX 4090 Laptop iÃ§in optimize edilmiÅŸ
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

# Logging ayarlarÄ±
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Torch Dynamo compiler'Ä± devre dÄ±ÅŸÄ± bÄ±rak
torch._dynamo.config.suppress_errors = True
torch._dynamo.config.disable = True

# RTX 4090 Laptop iÃ§in GPU optimizasyonlarÄ± - HIZLI KULLANIM
if torch.cuda.is_available():
    # GPU memory ayarlarÄ± - HIZLI memory kullan
    torch.cuda.empty_cache()
    torch.cuda.set_per_process_memory_fraction(0.85)  # GPU'nun %85'ini kullan - GÃœVENLÄ°
    torch.cuda.memory.empty_cache()
    gc.collect()
    
    # CUDA optimizasyonlarÄ±
    torch.backends.cudnn.benchmark = True  # HÄ±zlandÄ±rma
    torch.backends.cudnn.deterministic = False  # HÄ±z iÃ§in

class AdvancedMatematikRAG:
    def __init__(self, model_path: str = "gemma-2-9b-it-tr-new"):
        """
        Advanced Matematik RAG sistemi - FOTOGRAFTEKÄ° SÄ°STEME GÃ–RE DÃœZELTÄ°LDÄ°
        
        Args:
            model_path: Gemma model path
        """
        self.model_path = model_path
        self.embedding_model = None
        self.index = None
        self.documents = []
        self.embeddings = None
        self.llm = None
        self.tokenizer = None
        self.max_embedding_score = 0.0
        self.best_response = ""
        
        # Memory management arrays - 7 item limit
        self.user_messages = []  # KullanÄ±cÄ± mesajlarÄ± (max 7)
        self.bot_messages = []   # Bot mesajlarÄ± (max 7)
        
    def load_llm(self):
        """
        LLM model'ini yÃ¼kle - Gemma model iÃ§in optimize edilmiÅŸ
        """
        try:
            logger.info("Gemma LLM model yÃ¼kleniyor...")
            
            # Tokenizer'Ä± yÃ¼kle
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
            
            # Padding token ayarla
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # Model yÃ¼kleme ayarlarÄ± - GPU'ya yÃ¼kle (HIZLI)
            if torch.cuda.is_available():
                logger.info("ğŸ”„ GPU'da LLM yÃ¼kleniyor...")
                self.llm = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.float16,  # 16-bit precision - hÄ±z iÃ§in
                    device_map="auto",  # Otomatik GPU mapping
                    trust_remote_code=True,
                    low_cpu_mem_usage=True,
                    use_safetensors=True,
                    max_memory={0: "12GB"}  # GPU memory limiti - GÃœVENLÄ°
                )
            else:
                # CPU fallback
                self.llm = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.float32,
                    device_map="cpu",
                    trust_remote_code=True,
                    low_cpu_mem_usage=True,
                    use_safetensors=True
                )
            
            # Model'i evaluation moduna al
            self.llm.eval()
            
            logger.info("âœ… Gemma LLM model baÅŸarÄ±yla yÃ¼klendi")
            
        except Exception as e:
            logger.error(f"âŒ Gemma LLM model yÃ¼klenirken hata: {e}")
            raise
    
    def generate_with_llm(self, prompt: str, max_length: int = 200, temperature: float = 0.7) -> str:
        """
        LLM ile yanÄ±t Ã¼ret - KALÄ°TE VE HIZ OPTÄ°MÄ°ZASYONU
        
        Args:
            prompt: Input prompt
            max_length: Maksimum token uzunluÄŸu
            temperature: SÄ±caklÄ±k parametresi
            
        Returns:
            Ãœretilen yanÄ±t
        """
        try:
            # Memory temizle
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()
            
            # Tokenize
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
            
            # CPU'da iÅŸlemler (GPU sorunlarÄ± iÃ§in)
            # Input'lar zaten CPU'da
            # Model zaten CPU'da
            
            # Generate with optimized settings for RTX 4090 - KALÄ°TE VE HIZ DENGESÄ°
            with torch.no_grad():
                outputs = self.llm.generate(
                    **inputs,
                    max_new_tokens=max_length,
                    temperature=temperature,
                    top_p=0.9,
                    top_k=30,  # Biraz daha fazla Ã§eÅŸitlilik
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.encode("<end_of_turn>")[0] if "<end_of_turn>" in self.tokenizer.get_vocab() else self.tokenizer.eos_token_id,
                    repetition_penalty=1.1,
                    num_beams=1,  # HÄ±zlÄ± Ã¼retim
                    length_penalty=1.0,
                    no_repeat_ngram_size=2,
                    use_cache=True
                )
            
            # Decode
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Memory temizle
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # Temizle
            return self.preprocess_response(response)
            
        except Exception as e:
            logger.error(f"LLM Ã¼retiminde hata: {e}")
            return "ÃœzgÃ¼nÃ¼m, yanÄ±t Ã¼retirken bir hata oluÅŸtu."
    
    def generate_with_llm_fast(self, prompt: str, max_length: int = 50, temperature: float = 0.7) -> str:
        """
        LLM ile hÄ±zlÄ± yanÄ±t Ã¼ret - ESKÄ° SÄ°STEME GÃ–RE Ã‡OK HIZLI
        
        Args:
            prompt: Input prompt
            max_length: Maksimum token uzunluÄŸu (Ã§ok kÄ±sa)
            temperature: SÄ±caklÄ±k parametresi
            
        Returns:
            Ãœretilen yanÄ±t
        """
        try:
            # Tokenize - Ã‡OK KISA VE HIZLI
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=64)  # Daha kÄ±sa
            
            # Device kontrolÃ¼ - Model ve input aynÄ± device'da olmalÄ±
            if torch.cuda.is_available() and self.llm.device.type == "cuda":
                for key in inputs:
                    inputs[key] = inputs[key].to("cuda")
            else:
                for key in inputs:
                    inputs[key] = inputs[key].to("cpu")
            
            # Generate - Ã‡OK HIZLI AYARLAR
            with torch.no_grad():
                outputs = self.llm.generate(
                    **inputs,
                    max_new_tokens=max_length,
                    temperature=temperature,
                    top_p=0.8,  # Daha dÃ¼ÅŸÃ¼k - hÄ±z iÃ§in
                    top_k=10,    # Ã‡ok daha az - hÄ±z iÃ§in
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.05,  # Daha dÃ¼ÅŸÃ¼k - hÄ±z iÃ§in
                    num_beams=1,  # Greedy search - en hÄ±zlÄ±
                    use_cache=True,  # Cache kullan - hÄ±z iÃ§in
                    # early_stopping=True  # Bu parametre hataya neden oluyor
                )
            
            # Decode
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Temizle
            return self.preprocess_response(response)
            
        except Exception as e:
            logger.error(f"LLM Ã¼retiminde hata: {e}")
            return "ÃœzgÃ¼nÃ¼m, yanÄ±t Ã¼retirken bir hata oluÅŸtu."
    
    def preprocess_response(self, response):
        """
        Model yanÄ±tÄ±nÄ± temizle ve formatla
        
        Args:
            response: Ham model yanÄ±tÄ±
            
        Returns:
            TemizlenmiÅŸ yanÄ±t
        """
        # Model yanÄ±tÄ±nÄ± Ã§Ä±kar
        model_response = ""
        
        # "model" kelimesinden sonraki kÄ±smÄ± al
        if "model" in response:
            model_response = response.split("model")[-1].strip()
        else:
            model_response = response
        
        # Newline karakterlerini temizle ve tek satÄ±r yap
        model_response = model_response.replace('\n', ' ').replace('\r', ' ')
        
        # Fazla boÅŸluklarÄ± temizle
        model_response = re.sub(r'\s+', ' ', model_response).strip()
        
        # Gereksiz karakterleri temizle
        model_response = re.sub(r'\.{3,}', '', model_response)  # 3+ nokta
        model_response = re.sub(r'={3,}', '', model_response)   # 3+ eÅŸittir
        model_response = re.sub(r'-{3,}', '', model_response)   # 3+ tire
        model_response = re.sub(r'_{3,}', '', model_response)   # 3+ alt Ã§izgi
        model_response = re.sub(r'\*{3,}', '', model_response)  # 3+ yÄ±ldÄ±z
        model_response = re.sub(r'>{3,}', '', model_response)   # 3+ bÃ¼yÃ¼ktÃ¼r iÅŸareti
        model_response = re.sub(r'<{3,}', '', model_response)   # 3+ kÃ¼Ã§Ã¼ktÃ¼r iÅŸareti
        
        # Sonundaki gereksiz karakterleri temizle
        model_response = re.sub(r'[.\s]+$', '', model_response)  # Sonundaki nokta ve boÅŸluklar
        model_response = re.sub(r'^[.\s]+', '', model_response)  # BaÅŸÄ±ndaki nokta ve boÅŸluklar
        
        # Fazla boÅŸluklarÄ± tekrar temizle
        model_response = re.sub(r'\s+', ' ', model_response).strip()
        
        return model_response
    
    def create_embedding_for_text(self, text: str) -> np.ndarray:
        """
        Metin iÃ§in embedding oluÅŸtur
        
        Args:
            text: Input metin
            
        Returns:
            Embedding vektÃ¶rÃ¼
        """
        try:
            if self.embedding_model is None:
                # Embedding model'ini yÃ¼kle
                self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            
            # Embedding oluÅŸtur
            embedding = self.embedding_model.encode([text], convert_to_tensor=True)
            
            # CPU'ya taÅŸÄ± ve numpy array'e Ã§evir
            if torch.cuda.is_available():
                embedding = embedding.cpu().numpy()
            else:
                embedding = embedding.numpy()
            
            return embedding[0]  # Ä°lk (ve tek) embedding'i dÃ¶ndÃ¼r
            
        except Exception as e:
            logger.error(f"Embedding oluÅŸturma hatasÄ±: {e}")
            # Hata durumunda sÄ±fÄ±r vektÃ¶r dÃ¶ndÃ¼r
            return np.zeros(384)  # Default embedding boyutu
    
    def calculate_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """
        Ä°ki embedding arasÄ±ndaki benzerliÄŸi hesapla
        
        Args:
            embedding1: Ä°lk embedding
            embedding2: Ä°kinci embedding
            
        Returns:
            Benzerlik skoru (0-1 arasÄ±)
        """
        try:
            # Cosine similarity hesapla
            similarity = np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))
            return float(similarity)
        except Exception as e:
            logger.error(f"Benzerlik hesaplama hatasÄ±: {e}")
            return 0.0
    
    def pre_process_query(self, query: str) -> str:
        """
        Sorguyu Ã¶n iÅŸle (Pre-process adÄ±mÄ± - FOTOGRAFTEKÄ° SÄ°STEME GÃ–RE)
        
        Args:
            query: Orijinal sorgu
            
        Returns:
            Ä°ÅŸlenmiÅŸ sorgu
        """
        try:
            # LLM ile sorgu Ã¶zetleme/optimizasyonu
            prompt = f"""AÅŸaÄŸÄ±daki sorguyu matematik konularÄ± iÃ§in optimize et ve Ã¶zetle:

Orijinal sorgu: {query}

Sadece matematikle ilgili anahtar kelimeleri Ã§Ä±kar ve kÄ±sa bir Ã¶zet yap.
Ã–rnek: "toplama iÅŸlemi nasÄ±l yapÄ±lÄ±r" â†’ "toplama iÅŸlemi"
Ã–rnek: "kesirlerde Ã§arpma" â†’ "kesir Ã§arpma"
Ã–rnek: "geometri alan hesaplama" â†’ "geometri alan"

Optimize edilmiÅŸ sorgu:"""
            
            optimized = self.generate_with_llm(prompt, max_length=50, temperature=0.3)
            
            # EÄŸer LLM yanÄ±t vermezse orijinal sorguyu kullan
            if not optimized or len(optimized.strip()) < 3:
                return query
            
            return optimized.strip()
            
        except Exception as e:
            logger.error(f"Query preprocessing hatasÄ±: {e}")
            return query
    
    def filter_relevant_documents(self, query: str, retrieved_docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Ä°lgili dokÃ¼manlarÄ± filtrele (Filter adÄ±mÄ± - FOTOGRAFTEKÄ° SÄ°STEME GÃ–RE)
        
        Args:
            query: KullanÄ±cÄ± sorgusu
            retrieved_docs: Bulunan dokÃ¼manlar
            
        Returns:
            FiltrelenmiÅŸ dokÃ¼manlar
        """
        try:
            if not retrieved_docs:
                return []
            
            # DokÃ¼manlarÄ± string'e Ã§evir
            doc_texts = []
            for i, doc in enumerate(retrieved_docs):
                if isinstance(doc, dict):
                    doc_text = doc.get('document', str(doc))
                else:
                    doc_text = str(doc)
                doc_texts.append(f"DokÃ¼man {i+1}: {doc_text[:200]}...")
            
            docs_text = "\n".join(doc_texts)
            
            # LLM ile filtreleme
            prompt = f"""AÅŸaÄŸÄ±daki sorgu iÃ§in en uygun dokÃ¼manlarÄ± seÃ§:

Sorgu: {query}

Mevcut dokÃ¼manlar:
{docs_text}

Hangi dokÃ¼manlar bu sorgu iÃ§in en uygun? Sadece dokÃ¼man numaralarÄ±nÄ± yaz (Ã¶rn: 1, 3, 5)
Uygun dokÃ¼man numaralarÄ±:"""
            
            filter_response = self.generate_with_llm(prompt, max_length=50, temperature=0.3)
            
            # NumaralarÄ± Ã§Ä±kar
            numbers = re.findall(r'\d+', filter_response)
            
            if not numbers:
                # EÄŸer LLM yanÄ±t vermezse ilk 3 dokÃ¼manÄ± al
                return retrieved_docs[:3]
            
            # SeÃ§ilen dokÃ¼manlarÄ± dÃ¶ndÃ¼r
            selected_docs = []
            for num in numbers:
                idx = int(num) - 1
                if 0 <= idx < len(retrieved_docs):
                    selected_docs.append(retrieved_docs[idx])
            
            return selected_docs if selected_docs else retrieved_docs[:3]
            
        except Exception as e:
            logger.error(f"Document filtering hatasÄ±: {e}")
            return retrieved_docs[:3]  # Hata durumunda ilk 3 dokÃ¼manÄ± al
    
    def self_reflect_and_improve_with_embeddings(self, query: str, initial_response: str, relevant_docs: List[Dict[str, Any]]) -> str:
        """
        Basit cevap iyileÅŸtirme - Ã‡OK HIZLI VERSÄ°YON
        
        Args:
            query: Orijinal sorgu
            initial_response: Ä°lk yanÄ±t
            relevant_docs: Ä°lgili dokÃ¼manlar
            
        Returns:
            Cevap
        """
        # Basit cevap dÃ¶ndÃ¼r - Ã‡OK HIZLI
        return initial_response
    
    def self_reflect_and_improve(self, query: str, initial_response: str) -> str:
        """
        Basit cevap kontrolÃ¼ - Ã‡OK HIZLI VERSÄ°YON
        
        Args:
            query: KullanÄ±cÄ± sorgusu
            initial_response: Ä°lk yanÄ±t
            
        Returns:
            Cevap
        """
        # Basit cevap dÃ¶ndÃ¼r - Ã‡OK HIZLI
        return initial_response
    
    def load_data(self, json_files: List[str]) -> List[Dict[str, Any]]:
        """
        JSON dosyalarÄ±ndan veri yÃ¼kle
        
        Args:
            json_files: JSON dosya yollarÄ±
            
        Returns:
            YÃ¼klenen veri
        """
        data = []
        
        for file_path in json_files:
            try:
                if os.path.exists(file_path):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        file_data = json.load(f)
                        data.extend(file_data)
                        logger.info(f"{file_path} dosyasÄ± yÃ¼klendi: {len(file_data)} kayÄ±t")
                else:
                    logger.warning(f"{file_path} dosyasÄ± bulunamadÄ±")
            except Exception as e:
                logger.error(f"{file_path} dosyasÄ± yÃ¼klenirken hata: {e}")
        
        logger.info(f"Toplam {len(data)} kayÄ±t yÃ¼klendi")
        return data
    
    def prepare_documents(self, data: List[Dict[str, Any]]) -> List[str]:
        """
        Veriyi dokÃ¼man formatÄ±na Ã§evir
        
        Args:
            data: Ham veri
            
        Returns:
            DokÃ¼man listesi
        """
        documents = []
        
        for item in data:
            try:
                # JSON yapÄ±sÄ±na gÃ¶re dokÃ¼man oluÅŸtur
                if isinstance(item, dict):
                    # Prompt ve response alanlarÄ±nÄ± birleÅŸtir
                    prompt = item.get('prompt', '')
                    response = item.get('response', '')
                    
                    # DokÃ¼man oluÅŸtur
                    document = f"Soru: {prompt}\nCevap: {response}"
                    documents.append(document)
                else:
                    # String ise direkt ekle
                    documents.append(str(item))
                    
            except Exception as e:
                logger.error(f"DokÃ¼man hazÄ±rlama hatasÄ±: {e}")
                continue
        
        logger.info(f"{len(documents)} dokÃ¼man hazÄ±rlandÄ±")
        return documents
    
    def create_embeddings(self, documents: List[str], model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
        """
        DokÃ¼manlar iÃ§in embedding'ler oluÅŸtur
        
        Args:
            documents: DokÃ¼man listesi
            model_name: Embedding model adÄ±
        """
        try:
            logger.info("Embedding model yÃ¼kleniyor...")
            self.embedding_model = SentenceTransformer(model_name)
            
            logger.info("Embedding'ler oluÅŸturuluyor...")
            self.embeddings = self.embedding_model.encode(documents, convert_to_tensor=True)
            
            # CPU'ya taÅŸÄ± ve numpy array'e Ã§evir
            if torch.cuda.is_available():
                self.embeddings = self.embeddings.cpu().numpy()
            else:
                self.embeddings = self.embeddings.numpy()
            
            logger.info(f"Embedding'ler oluÅŸturuldu: {self.embeddings.shape}")
            
        except Exception as e:
            logger.error(f"Embedding oluÅŸturma hatasÄ±: {e}")
            raise
    
    def build_faiss_index(self, embeddings: np.ndarray):
        """
        FAISS index oluÅŸtur
        
        Args:
            embeddings: Embedding'ler
        """
        try:
            logger.info("FAISS index oluÅŸturuluyor...")
            
            # FAISS index oluÅŸtur
            dimension = embeddings.shape[1]
            self.index = faiss.IndexFlatIP(dimension)  # Inner Product (Cosine similarity)
            
            # Embedding'leri normalize et
            faiss.normalize_L2(embeddings)
            
            # Index'e ekle
            self.index.add(embeddings.astype('float32'))
            
            logger.info(f"FAISS index oluÅŸturuldu: {self.index.ntotal} vektÃ¶r")
            
        except Exception as e:
            logger.error(f"FAISS index oluÅŸturma hatasÄ±: {e}")
            raise
    
    def retrieve_relevant_documents(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        Ä°lgili dokÃ¼manlarÄ± getir
        
        Args:
            query: Sorgu
            top_k: DÃ¶ndÃ¼rÃ¼lecek dokÃ¼man sayÄ±sÄ±
            
        Returns:
            Ä°lgili dokÃ¼manlar
        """
        try:
            # Query embedding oluÅŸtur
            query_embedding = self.create_embedding_for_text(query)
            query_embedding = query_embedding.reshape(1, -1).astype('float32')
            
            # Normalize et
            faiss.normalize_L2(query_embedding)
            
            # FAISS ile arama yap
            scores, indices = self.index.search(query_embedding, top_k)
            
            # SonuÃ§larÄ± formatla
            results = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx < len(self.documents):
                    results.append({
                        'document': self.documents[idx],
                        'score': float(score),
                        'index': int(idx)
                    })
            
            # En yÃ¼ksek skoru kaydet
            if results:
                self.max_embedding_score = max(result['score'] for result in results)
            
            return results
            
        except Exception as e:
            logger.error(f"DokÃ¼man arama hatasÄ±: {e}")
            return []
    
    def create_context_prompt(self, query: str, relevant_docs: list, max_context_length: int = 2000) -> str:
        """
        Context prompt oluÅŸtur
        
        Args:
            query: Sorgu
            relevant_docs: Ä°lgili dokÃ¼manlar
            max_context_length: Maksimum context uzunluÄŸu
            
        Returns:
            Context prompt
        """
        try:
            # DokÃ¼manlarÄ± birleÅŸtir
            context = ""
            for i, doc in enumerate(relevant_docs):
                if isinstance(doc, dict):
                    doc_text = doc.get('document', str(doc))
                else:
                    doc_text = str(doc)
                context += f"Bilgi {i+1}: {doc_text}\n\n"
            
            # Gemma RAG prompt formatÄ±
            prompt = f"""<start_of_turn>user
AÅŸaÄŸÄ±daki matematik sorusunu cevapla:

SORU: {query}

BÄ°LGÄ°LER:
{context}

LÃ¼tfen verilen bilgileri kullanarak soruyu doÄŸru ve anlaÅŸÄ±lÄ±r ÅŸekilde cevapla. EÄŸer bilgiler yeterli deÄŸilse, kendi matematik bilginle yardÄ±mcÄ± ol. TÃ¼rkÃ§e olarak, Ã§ocuklarÄ±n anlayabileceÄŸi ÅŸekilde aÃ§Ä±kla. AdÄ±m adÄ±m Ã§Ã¶zÃ¼m gÃ¶ster. 

EÄŸer kullanÄ±cÄ± bir matematik sorusu istiyorsa (Ã¶rneÄŸin "rasyonel sayÄ±lar sorar mÄ±sÄ±n", "denklem sorusu sorar mÄ±sÄ±n"), o zaman verilen bilgilerden yola Ã§Ä±karak uygun bir matematik sorusu Ã¼ret. Soru Ã¼retirken:
- SÄ±nÄ±f seviyesine uygun olmalÄ±
- AnlaÅŸÄ±lÄ±r ve net olmalÄ±
- Ã‡Ã¶zÃ¼lebilir olmalÄ±
- Sadece soruyu ver, cevabÄ±nÄ± verme

EÄŸer kullanÄ±cÄ± bir soruyu cevaplamak istiyorsa, o zaman adÄ±m adÄ±m Ã§Ã¶zÃ¼m gÃ¶ster.
<end_of_turn>
<start_of_turn>model
"""
            
            return prompt
            
        except Exception as e:
            logger.error(f"Context prompt oluÅŸturma hatasÄ±: {e}")
            return f"LÃ¼tfen ÅŸu soruyu cevapla: {query}"
    
    def advanced_rag_pipeline(self, query: str, top_k: int = 5) -> str:
        """
        Advanced RAG pipeline - Ã‡OK HIZLI VERSÄ°YON
        
        Args:
            query: KullanÄ±cÄ± sorgusu
            top_k: DÃ¶ndÃ¼rÃ¼lecek dokÃ¼man sayÄ±sÄ±
            
        Returns:
            Model cevabÄ±
        """
        logger.info("Advanced RAG pipeline baÅŸlatÄ±lÄ±yor...")
        
        # 1. Basit sorgu optimizasyonu - Ã‡OK HIZLI
        logger.info("1. Sorgu optimizasyonu...")
        optimized_query = self.pre_process_query(query)
        
        # 2. DokÃ¼man arama - Ã‡OK HIZLI
        logger.info("2. DokÃ¼man arama...")
        retrieved_docs = self.retrieve_relevant_documents(optimized_query, top_k=1)  # Sadece 1 dokÃ¼man
        
        if not retrieved_docs:
            logger.info("DokÃ¼man bulunamadÄ±, LLM ile cevap Ã¼retiliyor...")
            return self.generate_llm_only_response(query)
        
        # 3. Basit prompt oluÅŸtur - Ã‡OK HIZLI
        logger.info("3. Prompt oluÅŸturma...")
        context_prompt = self.create_context_prompt(query, retrieved_docs, max_context_length=1000)
        
        # 4. HÄ±zlÄ± cevap Ã¼ret - Ã‡OK KISA
        logger.info("4. Cevap Ã¼retimi...")
        response = self.generate_with_llm_fast(context_prompt, max_length=50, temperature=0.7)
        
        logger.info("Advanced RAG pipeline tamamlandÄ±!")
        return response
    
    def select_best_response_by_embedding(self, query: str, current_response: str, relevant_docs: List[Dict[str, Any]]) -> str:
        """
        Basit cevap seÃ§imi - Ã‡OK HIZLI VERSÄ°YON
        
        Args:
            query: KullanÄ±cÄ± sorgusu
            current_response: Mevcut cevap
            relevant_docs: Ä°lgili dokÃ¼manlar
            
        Returns:
            Cevap
        """
        # Basit cevap dÃ¶ndÃ¼r - Ã‡OK HIZLI
        return current_response
    
    def calculate_response_embedding_score(self, query: str, response: str, relevant_docs: List[Dict[str, Any]]) -> float:
        """
        CevabÄ±n embedding skorunu hesapla - BASÄ°T VERSÄ°YON
        
        Args:
            query: KullanÄ±cÄ± sorgusu
            response: Cevap
            relevant_docs: Ä°lgili dokÃ¼manlar
            
        Returns:
            Embedding skoru (0-1 arasÄ±)
        """
        # Basit skor hesaplama - Ã‡OK HIZLI
        try:
            # Sadece sorgu ve cevap iÃ§in embedding hesapla
            query_embedding = self.create_embedding_for_text(query)
            response_embedding = self.create_embedding_for_text(response)
            
            # Basit benzerlik hesapla
            similarity = self.calculate_similarity(query_embedding, response_embedding)
            
            return similarity
            
        except Exception as e:
            logger.error(f"Embedding skoru hesaplanÄ±rken hata: {e}")
            return 0.5  # VarsayÄ±lan skor
    
    def save_advanced_rag_system(self, output_dir: str = "advanced_rag_system"):
        """
        Advanced RAG sistemini kaydet
        
        Args:
            output_dir: Ã‡Ä±ktÄ± dizini
        """
        try:
            # Dizini oluÅŸtur
            os.makedirs(output_dir, exist_ok=True)
            
            # Embedding model'ini kaydet
            if self.embedding_model is not None:
                embedding_dir = os.path.join(output_dir, "embedding_model")
                os.makedirs(embedding_dir, exist_ok=True)
                self.embedding_model.save(embedding_dir)
                logger.info("Embedding model kaydedildi")
            
            # FAISS index'i kaydet
            if self.index is not None:
                faiss.write_index(self.index, os.path.join(output_dir, "faiss_index.bin"))
                logger.info("FAISS index kaydedildi")
            
            # DokÃ¼manlarÄ± kaydet
            if self.documents:
                with open(os.path.join(output_dir, "documents.pkl"), 'wb') as f:
                    pickle.dump(self.documents, f)
                logger.info("DokÃ¼manlar kaydedildi")
            
            # Embedding'leri kaydet
            if self.embeddings is not None:
                np.save(os.path.join(output_dir, "embeddings.npy"), self.embeddings)
                logger.info("Embedding'ler kaydedildi")
            
            # LLM ve tokenizer'Ä± kaydet
            if self.llm is not None and self.tokenizer is not None:
                llm_dir = os.path.join(output_dir, "llm_model")
                os.makedirs(llm_dir, exist_ok=True)
                self.llm.save_pretrained(llm_dir)
                self.tokenizer.save_pretrained(llm_dir)
                logger.info("LLM model kaydedildi")
            
            logger.info(f"Advanced RAG sistemi {output_dir} dizinine kaydedildi")
            
        except Exception as e:
            logger.error(f"Sistem kaydetme hatasÄ±: {e}")
            raise
    
    def load_advanced_rag_system(self, input_dir: str = "advanced_rag_system"):
        """
        Advanced RAG sistemini yÃ¼kle
        
        Args:
            input_dir: GiriÅŸ dizini
        """
        try:
            # Embedding model'ini yÃ¼kle
            embedding_dir = os.path.join(input_dir, "embedding_model")
            if os.path.exists(embedding_dir):
                self.embedding_model = SentenceTransformer(embedding_dir)
                logger.info("Embedding model yÃ¼klendi")
            
            # FAISS index'i yÃ¼kle
            faiss_path = os.path.join(input_dir, "faiss_index.bin")
            if os.path.exists(faiss_path):
                self.index = faiss.read_index(faiss_path)
                logger.info("FAISS index yÃ¼klendi")
            
            # DokÃ¼manlarÄ± yÃ¼kle
            docs_path = os.path.join(input_dir, "documents.pkl")
            if os.path.exists(docs_path):
                with open(docs_path, 'rb') as f:
                    self.documents = pickle.load(f)
                logger.info("DokÃ¼manlar yÃ¼klendi")
            
            # Embedding'leri yÃ¼kle
            embeddings_path = os.path.join(input_dir, "embeddings.npy")
            if os.path.exists(embeddings_path):
                self.embeddings = np.load(embeddings_path)
                logger.info("Embedding'ler yÃ¼klendi")
            
            # LLM ve tokenizer'Ä± yÃ¼kle
            llm_dir = os.path.join(input_dir, "llm_model")
            if os.path.exists(llm_dir):
                self.llm = AutoModelForCausalLM.from_pretrained(llm_dir, trust_remote_code=True)
                self.tokenizer = AutoTokenizer.from_pretrained(llm_dir, trust_remote_code=True)
                self.llm.eval()
                logger.info("LLM model yÃ¼klendi")
            else:
                # LLM yoksa yÃ¼kle
                self.load_llm()
            
            logger.info(f"Advanced RAG sistemi {input_dir} dizininden yÃ¼klendi")
            
        except Exception as e:
            logger.error(f"Sistem yÃ¼kleme hatasÄ±: {e}")
            raise
    
    def train_advanced_rag_system(self, json_files: List[str], output_dir: str = "advanced_rag_system"):
        """
        Advanced RAG sistemini eÄŸit - FOTOGRAFTEKÄ° SÄ°STEME GÃ–RE DÃœZELTÄ°LDÄ°
        
        Args:
            json_files: JSON dosya yollarÄ±
            output_dir: Ã‡Ä±ktÄ± dizini
        """
        logger.info("Advanced RAG sistemi eÄŸitimi baÅŸlÄ±yor...")
        
        # 1. INQUIRY (Veri yÃ¼kleme)
        logger.info("1ï¸âƒ£ Inquiry (Veri yÃ¼kleme)...")
        data = self.load_data(json_files)
        
        # 2. PRE-PROCESS (DokÃ¼man hazÄ±rlama)
        logger.info("2ï¸âƒ£ Pre-process (DokÃ¼man hazÄ±rlama)...")
        self.documents = self.prepare_documents(data)
        
        # 3. EMBED (Embedding oluÅŸturma)
        logger.info("3ï¸âƒ£ Embed (Embedding oluÅŸturma)...")
        self.create_embeddings(self.documents)
        
        # 4. SEARCH (FAISS index oluÅŸturma)
        logger.info("4ï¸âƒ£ Search (FAISS index oluÅŸturma)...")
        self.build_faiss_index(self.embeddings)
        
        # 5. FILTER (LLM yÃ¼kleme)
        logger.info("5ï¸âƒ£ Filter (LLM yÃ¼kleme)...")
        self.load_llm()
        
        # 6. BUILD PROMPT (Sistem kaydetme)
        logger.info("6ï¸âƒ£ Build Prompt (Sistem kaydetme)...")
        self.save_advanced_rag_system(output_dir)
        
        # 7. GENERATE (EÄŸitim tamamlandÄ±)
        logger.info("7ï¸âƒ£ Generate (EÄŸitim tamamlandÄ±)...")
        
        # 8. SELF-REFLECT (Kalite kontrolÃ¼)
        logger.info("8ï¸âƒ£ Self-reflect (Kalite kontrolÃ¼)...")
        logger.info("Advanced RAG sistemi eÄŸitimi baÅŸarÄ±yla tamamlandÄ±!")
    
    def add_to_memory(self, user_message: str, bot_message: str):
        """
        Memory'ye mesaj ekle (7 item limit)
        
        Args:
            user_message: KullanÄ±cÄ± mesajÄ±
            bot_message: Bot mesajÄ±
        """
        # KullanÄ±cÄ± mesajÄ±nÄ± ekle
        self.user_messages.append(user_message)
        if len(self.user_messages) > 7:
            self.user_messages.pop(0)
        
        # Bot mesajÄ±nÄ± ekle
        self.bot_messages.append(bot_message)
        if len(self.bot_messages) > 7:
            self.bot_messages.pop(0)
    
    def get_memory_context(self) -> str:
        """
        Memory'den context oluÅŸtur
        
        Returns:
            Memory context
        """
        if not self.user_messages:
            return ""
        
        context = "Ã–nceki konuÅŸma:\n"
        for i in range(len(self.user_messages)):
            context += f"KullanÄ±cÄ±: {self.user_messages[i]}\n"
            if i < len(self.bot_messages):
                context += f"Bot: {self.bot_messages[i]}\n"
        context += "\nÅimdi sadece bu soruya cevap ver:\n"
        
        return context
    
    def generate_llm_only_response(self, query: str) -> str:
        """
        Sadece LLM ile yanÄ±t Ã¼ret (RAG olmadan)
        
        Args:
            query: KullanÄ±cÄ± sorgusu
            
        Returns:
            Bot yanÄ±tÄ±
        """
        try:
            # Memory context'i al
            memory_context = self.get_memory_context()
            
            # Gemma iÃ§in dÃ¼zeltilmiÅŸ prompt formatÄ±
            if memory_context:
                prompt = f"<start_of_turn>user\n{memory_context}KullanÄ±cÄ±: {query}<end_of_turn>\n<start_of_turn>model\n"
            else:
                prompt = f"<start_of_turn>user\n{query}<end_of_turn>\n<start_of_turn>model\n"
            
            # LLM ile yanÄ±t Ã¼ret
            response = self.generate_with_llm(prompt)
            
            return response
            
        except Exception as e:
            logger.error(f"LLM chat sÄ±rasÄ±nda hata: {e}")
            return "ÃœzgÃ¼nÃ¼m, yanÄ±t Ã¼retirken bir hata oluÅŸtu."
    
    def calculate_response_embedding_score(self, query: str, response: str, relevant_docs: List[Dict[str, Any]]) -> float:
        """
        YanÄ±tÄ±n embedding skorunu hesapla
        
        Args:
            query: Sorgu
            response: YanÄ±t
            relevant_docs: Ä°lgili dokÃ¼manlar
            
        Returns:
            Embedding skoru
        """
        try:
            # Query ve response embedding'lerini oluÅŸtur
            query_embedding = self.create_embedding_for_text(query)
            response_embedding = self.create_embedding_for_text(response)
            
            # Query-response benzerliÄŸi
            query_response_similarity = self.calculate_similarity(query_embedding, response_embedding)
            
            # Response-dokÃ¼man benzerlikleri
            doc_similarities = []
            for doc in relevant_docs:
                if isinstance(doc, dict):
                    doc_text = doc.get('document', str(doc))
                else:
                    doc_text = str(doc)
                
                doc_embedding = self.create_embedding_for_text(doc_text)
                doc_similarity = self.calculate_similarity(response_embedding, doc_embedding)
                doc_similarities.append(doc_similarity)
            
            # Ortalama dokÃ¼man benzerliÄŸi
            avg_doc_similarity = np.mean(doc_similarities) if doc_similarities else 0.0
            
            # Genel skor (query-response ve response-doc benzerliklerinin ortalamasÄ±)
            overall_score = (query_response_similarity + avg_doc_similarity) / 2
            
            return overall_score
            
        except Exception as e:
            logger.error(f"Embedding skor hesaplama hatasÄ±: {e}")
            return 0.0

def main():
    """Ana fonksiyon"""
    try:
        # JSON dosyalarÄ±
        json_files = [
            "mat1.json",
            "mat2.json", 
            "mat3.json",
            "mat4.json",
            "mat5.json",
            "mat6.json",
            "mat7.json",
            "mat8.json",
            "mat8_lgs.json"
        ]
        
        # Mevcut dosyalarÄ± kontrol et
        existing_files = []
        for file in json_files:
            if os.path.exists(file):
                existing_files.append(file)
            else:
                print(f"âš ï¸  {file} dosyasÄ± bulunamadÄ±, atlanÄ±yor...")
        
        if not existing_files:
            print("âŒ HiÃ§bir JSON dosyasÄ± bulunamadÄ±! RAG sistemi eÄŸitilemiyor.")
            return
        
        print(f"ğŸ“š {len(existing_files)} JSON dosyasÄ± bulundu. Advanced RAG sistemi eÄŸitiliyor...")
        
        # Advanced RAG sistemi oluÅŸtur ve eÄŸit
        advanced_rag = AdvancedMatematikRAG()
        advanced_rag.train_advanced_rag_system(existing_files, "advanced_rag_system")
        
        print("âœ… Advanced RAG sistemi baÅŸarÄ±yla eÄŸitildi!")
        
    except Exception as e:
        print(f"âŒ Advanced RAG sistemi eÄŸitilirken hata oluÅŸtu: {e}")

if __name__ == "__main__":
    main() 